{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Equation of Tranfer:\n",
    "\n",
    "![Robot Maze](https://datawhalechina.github.io/joyrl-book/figs/ch3/robot_maze_2.png)\n",
    "\n",
    "$$\n",
    "f(i,j) = \\begin{cases}\n",
    "0, & (i,j) = (0,0) \\\\\n",
    "1, & i = 0 \\text{ or } j = 0, (i,j) \\neq (0,0) \\\\\n",
    "f(i-1,j) + f(i,j-1), & i > 0, j > 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.62e+26'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Implementation'''\n",
    "def solve(m,n):\n",
    "    # initialize the border condition\n",
    "    f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] \n",
    "    \n",
    "    # transfer\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            f[i][j] = f[i - 1][j] + f[i][j - 1]\n",
    "            \n",
    "    return \"{:.2e}\".format(f[m - 1][n - 1])\n",
    "\n",
    "'''Example'''\n",
    "solve(32, 76) # How many paths are there through a maze of 32*76?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State-Value Function\n",
    "The expected sum of discounted rewards from a state $s$ (like discounted cash flow)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(s) &= \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n",
    "     &= R(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s' \\mid s) V_\\pi(s')\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "##### Action-Value Function\n",
    "Introduce the action $a$ into the state-value function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s,a) &= \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] \\\\\n",
    "       &= R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s' \\mid s, a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s') Q(s',a')\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Evidently,\n",
    "$$\n",
    "V(s) = \\sum_{a \\in A} \\pi(a|s)Q(s,a)\n",
    "$$\n",
    "in which $\\pi(a|s)$ means the probability distribution of action $a$ in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedy Policy\n",
    "\n",
    "Define the policy $\\pi$ as a function of state $s$\n",
    "$$\n",
    "\\pi(a|s) = \\max_{a} Q(s,a)\n",
    "$$\n",
    "\n",
    "### Value Iteration vs Policy Iteration\n",
    "Value Iteration uses the Bellman Optimality Equation\n",
    "$$\n",
    "V_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r(s,a,s') + \\gamma V_k(s') \\right]\n",
    "$$\n",
    "While Policy Iteration uses the Bellman Expectation Equation plus Policy Improvement\n",
    "$$\n",
    "V^{\\pi_k}(s) = \\sum_{s',r} p(s',r|s,\\pi_k(s)) \\left[ r(s,a,s') + \\gamma V^{\\pi_k}(s') \\right]\\\\\n",
    "\\text{then\\,\\,}\\pi_{k+1}(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a) \\left[ r(s,a,s') + \\gamma V^{\\pi_k}(s') \\right]\n",
    "$$\n",
    "##### Which is better (faster)?\n",
    "Policy Iteration converges by alternating between policy evaluation and policy improvement. The altering process is almost instantaneous, as it only involves \n",
    "calculating the value function and the policy function in turn using a single equation.\n",
    "\n",
    "Value Iteration converges by iteratively applying the Bellman Optimality Equation until it converges to the optimal value function. Each iteration involves calculating the value function for multiple states, which mostly takes longer than the policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Three Properties of Normal Dynamic Planning\n",
    "1. Principle of Optimality:     \n",
    "An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n",
    "2. Memeryless Property (or Markov Property):       \n",
    "The future state depends only on the current state, not on the sequence of events that preceded it.    \n",
    "3. Overlapping Subproblems:\n",
    "The same subproblems are solved multiple times in a naive recursive approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
