{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction vs Control\n",
    "| Aspect      | Prediction                                      | Control                                 |\n",
    "|-------------|-------------------------------------------------|-----------------------------------------|\n",
    "| Goal        | Evaluate how good the current policy is         | Find the optimal policy                 |\n",
    "| Question    | Given a policy π, what is the value of state s? | What policy yields the maximum return?  |\n",
    "| Output      | Value function $V^\\pi(s)$ or $Q^\\pi(s, a)$ | Optimal policy $\\pi^*$              |\n",
    "\n",
    "### Model-Free vs Model-Based\n",
    "Model-based: I know the rules of the game. When I take an action, I know the outcome.\n",
    "Model-free: I don't know the rules of the game. I start from scratch and to directly to learn the behavior of the game.\n",
    "\n",
    "### Monte Carlo Estimation of Returns\n",
    "Monte Carlo: To know the area of an irregular shape, we can shy a lot of beans on it. The smaller the bean and the more beans we use, the more accurate the estimate will be.        \n",
    "First-Visit MC: Only the first time a state is iterated is counted.     \n",
    "   \n",
    "`For` each episode:<br>\n",
    "　　　`If` state $s$ is visited for the first time:<br>\n",
    "　　　　　　`Append` $G$ to Returns$(s)$<br>\n",
    "　　　　　　`Update`: $V(s) ← V(s) + α[G - V(s)]$<br>        \n",
    "\n",
    "Every-Visit MC: Every time a state is iterated is counted.\n",
    "\n",
    "`For` each episode:<br>\n",
    "　　　`For` each state $s$:<br>\n",
    "　　　　　　`Append` $G$ to Returns$(s)$<br>\n",
    "　　　　　　`Update`: $V(s) ← V(s) + α[G - V(s)]$<br>\n",
    "\n",
    "FVMC vs EVMC is another typical example of the efficiency-vs-accuracy trade-off in machine learning.\n",
    "\n",
    "### Temporal-Difference vs Monte Carlo\n",
    "| Feature                        | Temporal Difference (TD)                | Monte Carlo (MC)                      |\n",
    "|---------------------------------|-----------------------------------------|---------------------------------------|\n",
    "| Update Timing                  | Updates after every step (online)       | Updates after each episode (offline)  |\n",
    "| Bootstrapping                  | Yes (uses estimated value of next state)| No (uses actual returns only)         |\n",
    "| Sample Efficiency              | More sample efficient                   | Less sample efficient                 |\n",
    "| Variance                       | Lower variance                          | Higher variance                       |\n",
    "| Bias                           | May have bias due to bootstrapping      | Unbiased (if enough episodes)         |\n",
    "| Need for Episode End           | No (can learn from incomplete episodes) | Yes (needs complete episodes)         |\n",
    "| Example Algorithms             | SARSA, Q-learning, TD(0)                | First-Visit MC, Every-Visit MC        |\n",
    "| Convergence Speed              | Usually faster                          | Usually slower                        |\n",
    "| Applicability to Continuing Tasks | Yes                                  | No (typically for episodic tasks)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "Q-learning uses the maximum Q-value of the next state to update current action values, rather than the expected value under the actual policy. This optimistic assumption leads to overestimation bias.\n",
    "\n",
    "##### Exploitation vs Exploration and $\\epsilon$-greedy Policy\n",
    "Exploration: Each episode updates Q-values for all actions. It reflects a combination of TD and MC philosophies.\n",
    "Bootstrapping relies on the previous estimate of the Q-value. It's like people always sticks to the same path; the experience built up in the past can facilitate efficiency, but there will come up new problems that can't be solved with the old methods.      \n",
    "$\\epsilon$ can be regarded as how curious the agent is about the new path. If it's too high, the agent may forego the old path but it's not guaranteed to find another optimal path. So, $\\epsilon$ is usually set to $0.1$ or even lower.     \n",
    "Moreover, while the knowledge system becomes relatively complete, the probability of discovering something new gets lower, so $\\epsilon$ reduces over steps, e.g. from $0.1$ to $0.01$ after 200 steps.\n",
    "Exploration-Exploitation dilemma is very much like bias-variance trade-off in Deep Learning.\n",
    "\n",
    "\n",
    "\n",
    "### Sarsa\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "Sarsa usese the direct value of the next state to update the current action value.\n",
    "\n",
    "##### On-Policy vs Off-Policy\n",
    "Notably, the Sarsa algorithm generates data samples using the current policy during training and updates based on them. In other words, the policy evaluation and policy improvement processes are performed using the same policy, which is referred to as an on-policy algorithm. Correspondingly, algorithms such as Q-learning that obtain samples from other policies and use them to update the target policy are referred to as off-policy algorithms. In other words, off-policy algorithms essentially learn from experience pools or historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the training process\n",
    "\n",
    "for i_ep in range(train_eps): # iterate over episodes\n",
    "    # reset environment, get initial state\n",
    "    state = env.reset()  # reset environment, i.e. start a new episode\n",
    "    while True: # for complex games, we can set the maximum number of steps per episode,\n",
    "                # e.g. while ep_step<100, i.e. the maximum number of steps is 100.\n",
    "        # the agent samples an action according to the policy\n",
    "        action = agent.sample_action(state)  # sample an action\n",
    "        # interact with the environment, get the next state and reward\n",
    "        next_state, reward, terminated, _ = env.step(action)  # agent records the sample to the experience pool\n",
    "        agent.memory.push(state, action, reward, next_state, terminated) \n",
    "        # agent updates the policy\n",
    "        agent.update(state, action, reward, next_state, terminated)  \n",
    "        # update the state\n",
    "        state = next_state  \n",
    "        # if the episode is terminated, the current episode ends\n",
    "        if terminated:\n",
    "            break\n",
    "        \n",
    "        \n",
    "# 2. Define the algorithm\n",
    "# 2.1 Sample an action\n",
    "class Agent:\n",
    "    def __init__(self, n_actions):\n",
    "        self.Q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "        pass\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        self.sample_count += 1\n",
    "    \n",
    "        # epsilon decreases exponentially\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(- self.sample_count / self.epsilon_decay) \n",
    "        \n",
    "        # e-greedy strategy\n",
    "        if np.random.uniform(0, 1) > self.epsilon:\n",
    "            action = np.argmax(self.Q_table[str(state)]) # select the action with the highest Q-value\n",
    "        else:\n",
    "            action = np.random.choice(self.n_actions) # randomly select an action\n",
    "        return action\n",
    "    \n",
    "# 2.2 Predict an action\n",
    "    def predict_action(self,state):\n",
    "        action = np.argmax(self.Q_table[str(state)])\n",
    "        return action\n",
    "    \n",
    "# 2.3 Update the Q-table\n",
    "    def update(self, state, action, reward, next_state, terminated):\n",
    "        Q_predict = self.Q_table[str(state)][action] \n",
    "        if terminated: # for a terminated state\n",
    "            Q_target = reward  \n",
    "        else:\n",
    "            Q_target = reward + self.gamma * np.max(self.Q_table[str(next_state)]) \n",
    "        self.Q_table[str(state)][action] += self.lr * (Q_target - Q_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare the environment\n",
    "env = gym.make('CliffWalking-v0') # by OpenAI Gym\n",
    "n_states = env.observation_space.n # number of states\n",
    "n_actions = env.action_space.n # number of actions\n",
    "print(f\"Number of states: {n_states}, number of actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set parameters\n",
    "self.epsilon_start = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "self.epsilon_end = 0.01 #  e-greedy策略中epsilon的最终值\n",
    "self.epsilon_decay = 200 #  e-greedy策略中epsilon的衰减率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Ablation study\n",
    "# set the initial and final values to be the same, so that epsilon won't decay\n",
    "self.epsilon_start = 0.1\n",
    "self.epsilon_end = 0.1\n",
    "self.epsilon_decay = 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
